{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Attention Module**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras import backend as K, initializers, regularizers, constraints\nfrom keras.engine.topology import Layer\n\nclass Attention(Layer):\n  def __init__(self, step_dim, W_regularizer=None, b_regularizer=None,\n               W_constraint=None, b_constraint=None,\n               bias=True, return_attention=False, **kwargs):\n    \n    self.supports_masking = True\n    self.init = initializers.get('glorot_uniform')\n\n    self.W_regularizer = regularizers.get(W_regularizer)\n    self.b_regularizer = regularizers.get(b_regularizer)\n\n    self.W_constraint = regularizers.get(W_constraint)\n    self.b_constraint = regularizers.get(b_constraint)\n\n    self.step_dim = step_dim\n    self.features_dim = 0\n    self.bias = bias\n    super(Attention, self).__init__(**kwargs)\n\n  def build(self, input_shape):\n    assert len(input_shape) == 3\n\n    self.W = self.add_weight((input_shape[-1],),\n                             initializer=self.init,\n                             name='{}_W'.format(self.name),\n                             regularizer=self.W_regularizer,\n                             constraint=self.W_constraint)\n\n    self.features_dim = input_shape[-1]\n\n    if self.bias:\n      self.b = self.add_weight((input_shape[1],),\n                               initializer='zero',\n                               name='{}_b'.format(self.name),\n                               regularizer=self.b_regularizer,\n                               constraint=self.b_constraint)\n    else:\n      self.b = None\n\n    self.built = True\n  \n  def compute_mask(self, input, input_mask=None):\n    return None\n\n  def call(self, x, mask=None):\n    features_dim = self.features_dim\n    step_dim = self.step_dim\n\n    eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                          K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n    \n    if (self.bias):\n      eij += self.b\n\n    eij = K.tanh(eij)\n\n    a = K.exp(eij)\n\n    if mask is not None:\n      a += K.cast(mask, K.floatx())\n\n    a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n    weighted_input = x * K.expand_dims(a)\n\n    result = K.sum(weighted_input, axis=1)\n\n    if self.return_attention:\n      return [result, a]\n    \n    return result\n\n    def compute_output_shape(self, input_shape):\n      if self.return_attention:\n        return [(input_shape[0], input_shape[-1]),\n                (input_shape[0], input_shape[1])]\n      else:\n        return input_shape[0], input_shape[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Encoding Helpers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"aa_idx = {'A':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'K':9, \n            'L':10, 'M':11, 'N':12, 'P':13, 'Q':14, 'R':15, 'S':16, 'T':17, \n            'V':18, 'W':19, 'Y':20, '-':21}\n\ndef onehot(seq, k=15, mask=True):\n  s = list(seq)\n  if (len(s) < k):\n    s = s + (['-'] * (k - len(s)))\n  else:\n    s = s[0:k]\n  \n  vec = []\n  for let in range(k):\n    char = s[let]\n    row = np.random.uniform(low=0.001, high=0.01,size=21)\n    if (char not in aa_idx):\n      char = '-'\n    row[aa_idx[char]-1] = 1\n    if ((char == '-') and (mask)):\n      row = np.ones((21)) * (-400)\n    \n    vec.append(row)\n    \n  vec = np.array(vec).flatten()\n  return vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Generator Class**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import namedtuple\nfrom tensorflow.keras.utils import Sequence\n\nDATA_ENTRY = namedtuple(\"data_entry_ic50\", \"sequence, hla, y_val, pep_length\")\n\nclass DataGenerator(Sequence):\n  def __init__(self, batch_size, samples, y_cat, mask=True):\n        self.batch_size = batch_size\n        self.samples = samples\n        self.y_cat = y_cat\n        self.mask = mask\n        self.epitope_enc_map = {}\n        self.hla_enc_map = {}\n        self.init_data()\n        self.on_epoch_end()\n\n  def init_data(self):\n    for sample in self.samples:\n      sequence=sample[0]\n      hla=sample[1]\n      pep_length=int(sample[2])\n      self.epitope_enc_map[sequence] = (onehot(sequence, 9, self.mask)).reshape(1, 9, 21)\n      self.hla_enc_map[hla] = (onehot(hla, 34, mask)).reshape(1, 34, 21)\n\n  def __len__(self):\n    #return math.ceil(len(self.samples) / self.batch_size).astype(np.int)\n    return (np.ceil(len(self.samples) / float(self.batch_size))).astype(np.int)\n\n  def on_epoch_end(self):\n    #'Updates indexes after each epoch'\n    np.random.shuffle(self.samples)\n\n  def __getitem__(self, idx):\n    hla_alleles = []\n    epitopes = []\n    y_vals = []\n    batch_sample = self.samples[idx * self.batch_size : (idx+1) * self.batch_size]\n\n    i = 0\n    for sample in batch_sample:\n\n      y_val= float(sample[3])\n      sequence = sample[0]\n      hla = sample[1]\n\n      # protein\n      epitope_encoded = self.epitope_enc_map[sequence]\n      epitopes.append(epitope_encoded)\n\n      # hla\n      hla_encoded = self.hla_enc_map[hla]\n      hla_alleles.append(hla_encoded)\n\n      # log_ic50 or binder v non-binder or immunogenicity\n      if (self.y_cat != \"binding\"):\n        y_vals.append(int(y_val))\n      else:\n        y_vals.append(y_val)\n\n      i += 1\n\n    ret_in = {'epitope':np.array(epitopes), 'hla':np.array(hla_alleles)}\n    ret_out = np.array(y_vals)\n      \n    return (ret_in, [ret_out],)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MODEL 1: BiLSTM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import *\n#from keras import Input\n#from keras.layers import Bidrectional\n\ndef model_biLSTM():\n    x = Input(shape(40, 21), )\n    y = Bidirectional(CuDNNLSTM(units=64, return_sequences = True))(x)\n    y = Bidirectional(CuDNNLSTM(units=64, return_sequences = True))(y)\n    y = Attention(40)(y)\n    rel = Dense(64, activation=\"relu\")(y)\n    drop = Dropout(0.1)(reg)\n    out = Dense(1, activation=\"sigmoid\")(drop)\n    model = Model(inputs=x, outputs=out)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MODEL 2: LSTM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MASK_VALUE = -400\ndef model_LSTM():\n    x =  Input(shape(40, 21), )\n    y = Masking(mask_value=MASK_VALUE, input_shape=(x.shape))(x)\n    y = CuDNNLSTM(units=64, return_sequences = True)(y)\n    dense = Dense(64)(y)\n    drop = Dropout(0.2)(dense)\n    act = Activation('tanh')(drop)\n    out = Dense(1, activation='sigmoid')(act)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cross Validation Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom sklearn.model_selection import KFold\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.utils import Sequence\nfrom keras.initializers import glorot_uniform\n\n## NN Model is a model \n## training data in form of a df with 4 columns: Sequence, HLA, Y_val, Pep_Length\n## y --  either immunogenicity or binding scores\n## y_cat = either binding or else\n## outfile where you want to output data to\n## hla_encoding should be pseudo and NEVER full sequence\ndef train_gen_crossval(nn_model, training_data, y_cat, out_file, mask=True):\n    \n  f = open(out_file, \"w\")\n  # Split data into training, test sets\n  splits = list(KFold(n_splits=10, shuffle=True, random_state=20).split(training_data))\n\n  print('we splitted')\n    \n  for (i, (train_index, test_index)) in enumerate(splits):\n    keras.backend.clear_session()\n    # get train_data, validation_data\n\n    trains = training_data.iloc[train_index,:].values\n    train_samples, validation_samples = split_set(trains, 0.2)\n    print(\"split training set\")\n    \n    test_samples = training_data.iloc[test_index,:].values\n    print(\"got test samples\")\n  \n    # make generator out of train data, batch size=16\n    train_generator = DataGenerator(256, train_samples, y_cat, mask)\n    print(\"made training generator:\", train_generator)\n  \n    # make generator out of test data, batch size =16\n    validation_generator = DataGenerator(256, validation_samples, y_cat, mask)\n    print(\"made validation generator:\", validation_generator)\n\n    # call model() -- should be parametrized tho\n    model = nn_model()\n    \n    print(\"made model\")\n    # compile model\n    if (y_cat == 'binding'):\n      model.compile(optimizer=Adam(lr=0.001), loss=[\"mean_squared_error\"], metrics = ['mean_squared_error'])\n    else:\n      model.compile(optimizer=Adam(lr=0.001), loss=['binary_crossentropy'],\n                    metrics=[keras.metrics.BinaryAccuracy(), \n                             keras.metrics.TruePositives(),\n                             keras.metrics.TrueNegatives(), \n                             keras.metrics.FalsePositives(),\n                             keras.metrics.FalseNegatives()])\n\n    weights_path = './' + 'weights_{}.h5'.format(i)  \n    print(\"Model compiled\")\n    # checkpoint\n    ckpt = ModelCheckpoint(weights_path, save_best_only=True, \n                           save_weights_only=False, verbose=1, \n                           monitor='val_loss', mode='auto')\n    early = EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=0.001)\n    \n    model.train_generator = train_generator\n\n    model.fit(model.train_generator, epochs=1000, \n              steps_per_epoch=len(train_generator), \n              validation_steps=len(validation_generator),\n              validation_data=validation_generator, \n              callbacks=[ckpt, early])\n    # loads the best weights saved by the checkpoint\n    model.load_weights(weights_path)\n\n    hlas = []\n    epitopes = []\n    ys = []\n\n    for sample in test_samples:\n      hla_encoded = onehot(sample[1], 34).reshape(1, 34, 21)\n      hlas.append(hla_encoded)\n      epitope_encoded = onehot(sample[0], 9).reshape(1, 9, 21)\n      epitopes.append(epitope_encoded)\n      ys.append(sample[3])\n\n    results = model.predict({'epitope': np.array(epitopes),\n                             'hla': np.array(hlas)})\n    \n    results = np.array(results)\n    print(results.shape)\n    # write results\n    for i in range(len(ys)):\n        real = ys[i]\n        predict_y = results[i]\n        print(\"real: {}, predict: {}\".format(real, predict_y))\n        epitope = epitopes[i]\n        hla = hlas[i]\n        f.write(\"{},{},{},{}\\n\".format(hla, epitope, real, predict_y))\n        \n    f.close()\n\n    print(\"Finish fold {}...\".format(i))\n    print('\\n'*8)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cross Validation Helpers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_set(samples, ratio):\n  for _ in range(10):\n    np.random.shuffle(samples)\n  \n  train_count = math.ceil(len(samples) * (1 - ratio))\n  return samples[:train_count], samples[train_count:]\n\ndef to_np(arr):\n  X = []\n  for x in arr:\n    X.append(x)\n  X = np.array(X)\n  return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training code"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_binding=pd.read_csv(\"../input/binding-train-pseudo/binding_train_pseudo.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen_crossval(model_LSTM, df_train_binding, \"binding\", \"test_lstm_output.txt\", mask=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen_crossval(model_biLSTM, df_train_binding, \"binding\", \"test_bilstm_output.txt\", mask=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import *\nfrom keras.utils.generic_utils import CustomObjectScope\n\ndef main(binding_model, immunogenicity_model, out_file, test_X, test_y=None, \n         hla_encoding_method=\"pseudo\", attention=False):\n  if (attention):\n    with CustomObjectScope({'Attention': Attention}):\n      BA_model = load_model(binding_model)\n      IMM_model = load_model(immunogenicity_model)\n  else:\n    BA_model = load_model(binding_model)\n    IMM_model = load_model(immunogenicity_model)\n\n  outfile = open(\"%s.txt\" % (out_file), \"r\")\n  for sample in test_X:\n    hla = onehot(sample[0])\n    peptide = onehot(sample[1])\n    bind_out = BA_model.predict({\n        'protein': np.array([hla]),\n        'ligand': np.array([peptide]),\n        })\n    imm_out = IMM_model.predict({\n      'protein': np.array([hla]),\n      'ligand': np.array([peptide]), \n    })\n    outfile.write('{},{},{} (log_ic50),{} (binary)'.format(sample[0], sample[1], out[0][0][0], out[1][0][0]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}