{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Functions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"aa_idx = {'A':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'K':9, \n            'L':10, 'M':11, 'N':12, 'P':13, 'Q':14, 'R':15, 'S':16, 'T':17, \n            'V':18, 'W':19, 'Y':20, '-':21}\n\ndef onehot(seq, k=15):\n  s = list(seq)\n  if (len(s) < k):\n    s = s + (['-'] * (k - len(s)))\n  else:\n    s = s[0:k]\n  \n  vec = []\n  for let in range(k):\n    char = s[let]\n    row = np.random.uniform(low=0.001, high=0.01,size=21)\n    if (char not in aa_idx):\n      char = '-'\n    row[aa_idx[char]-1] = 1\n    vec.append(row)\n  vec = np.array(vec).flatten()\n  return vec\n\ndef softhot(seq, k=15):\n  s = list(seq)\n  if (len(s) < k):\n    s = s + (['-'] * (k - len(s)))\n  else:\n    s = s[0:k]\n  \n  vec = []\n  for let in range(k):\n    char = s[let]\n    row = np.ones((21)) * ((0.1)/(20)) \n    if (char not in aa_idx):\n      char = '-'\n    row[aa_idx[char]-1] = 0.9\n    vec.append(row)\n    if (char == '-'):\n      row = np.ones((21)) * (-400000)\n  vec = np.array(vec).flatten()\n  return vec\n\n# met refers to function\ndef encode_seq(method, peptide_seqs, k=15):\n  methods_dict = {'ONEHOT':onehot, 'BLOSUM':blosum, 'SOFTHOT':softhot}\n  met = methods_dict[method]\n\n  if (k == 15):\n    mapped = peptide_seqs.apply(lambda x: met(x))\n  else:\n    mapped = peptide_seqs.apply(lambda x: met(x, k))\n  return mapped\n\ndef pseudo_HLA(seq, ind):\n  seq = list(seq)\n  N = [ seq[i] for i in ind]\n  sequence = ''.join(N)\n  print(seq, sequence)\n  return sequence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Driver"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_training_full(y_cat, hla_encoding_method=\"pseudo\"):\n  \n  path = \"/content/drive/MyDrive/02-518/02518 Comp Medicine Project/Input Data Files\"\n  \n  final_df = pd.DataFrame(columns=[\"Sequence\", \"HLA\", \"Pep_Length\",\"Y_val\"])\n\n  if (y_cat == \"binding\"):\n    input_path = (\"%s/binding_dat.csv\" % (path))\n  else:\n    input_path = (\"%s/immunogenicity_dat.csv\" % (path))\n\n  input_df = pd.DataFrame(pd.read_csv(input_path, index_col=0))\n\n  final_df[\"Sequence\"] = input_df[\"Peptide\"]\n  final_df[\"Pep_Length\"] = input_df[\"Length\"]\n\n  HLA_pseudo = input_df[\"HLA\"].apply(lambda x: HLA_DICT[x]['HLA_Pseudo'])\n  \n  if (hla_encoding_method == \"pseudo\"):\n    final_df[\"HLA\"] = input_df[\"HLA\"]\n  else:\n    final_df[\"HLA\"] = HLA_pseudo\n\n  if (y_cat == \"binding\"):\n    final_df[\"Y_val\"] = input_df[\"IC50_transformed\"]\n  else:\n    final_df[\"Y_val\"] = input_df[\"Immunogenic\"]\n\n  return final_df\n\ndef prep_training(y_cat,hla_encoding_method):\n  df = prep_training_full(y_cat, hla_encoding_method)\n  X = df[[\"Sequence\", \"HLA\", \"Pep_Length\"]]\n  y = df[\"Y_val\"]\n\n  return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre-processed data formatted as [sequence, hla, pep_length]\n\nfrom collections import namedtuple\nfrom tensorflow.keras.utils import Sequence\n\nDATA_ENTRY = namedtuple(\"data_entry_ic50\", \"sequence, hla, y_val, pep_length\")\n\nclass DataGenerator(Sequence):\n  def __init__(self, batch_size, samples, y_cat):\n        self.batch_size = batch_size\n        self.samples = samples\n        self.y_cat = y_cat\n        self.epitope_enc_map = {}\n        self.hla_enc_map = {}\n        self.init_data()\n        self.on_epoch_end()\n\n  def init_data(self):\n    for sample in self.samples:\n      sequence=sample[0]\n      hla=sample[1]\n      pep_length=int(sample[2])\n      self.epitope_enc_map[sequence] = (onehot(sequence, 9)).reshape(1, 9, 21)\n      self.hla_enc_map[hla] = (onehot(hla, 34)).reshape(1, 34, 21)\n\n  def __len__(self):\n    return (np.ceil(len(self.samples) / float(self.batch_size))).astype(np.int)\n\n  def on_epoch_end(self):\n    #'Updates indexes after each epoch'\n    np.random.shuffle(self.samples)\n\n  def __getitem__(self, idx):\n    hla_alleles = []\n    epitopes = []\n    y_vals = []\n    batch_sample = self.samples[idx * self.batch_size : (idx+1) * self.batch_size]\n\n    i = 0\n    for sample in batch_sample:\n\n      y_val= float(sample[3])\n      sequence = sample[0]\n      hla = sample[1]\n\n      # protein\n      epitope_encoded = self.epitope_enc_map[sequence]\n      epitopes.append(epitope_encoded)\n\n      # hla\n      hla_encoded = self.hla_enc_map[hla]\n      hla_alleles.append(hla_encoded)\n\n      # log_ic50 or binder versus non-binder or immunogenic versus non-immunogenic\n      if (self.y_cat != \"binding\"):\n        y_vals.append(int(y_val))\n      else:\n        y_vals.append(y_val)\n\n      i += 1\n\n    ret_in = {'epitope':np.array(epitopes), 'hla':np.array(hla_alleles)}\n    ret_out = np.array(y_vals)\n      \n    return (ret_in, [ret_out],)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Convolutional Neural Net Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.utils import plot_model\nfrom keras.regularizers import *\n\ndef vgg_pool_block(layer, num_filter, max_pool=True):\n    # a block does a convolution, batch norm, and max pooling\n    layer = Conv2D(num_filter,\n           (1, 3),\n           strides=(1, 1),\n           kernel_regularizer=l2(l=0.1),\n           data_format='channels_last',\n           padding='same')(layer)\n\n    layer = BatchNormalization()(layer)\n    \n    layer = LeakyReLU()(layer)\n    \n    layer = Conv2D(num_filter,\n           (1, 3),\n           strides=(1, 1),\n           kernel_regularizer=l2(l=0.1),\n           data_format='channels_last',\n           padding='same')(layer)\n\n    layer = BatchNormalization()(layer)\n    \n    layer = LeakyReLU()(layer)\n    \n    if max_pool:\n        layer = MaxPooling2D(pool_size=(1,2))(layer)\n    \n        layer = Dropout(0.5)(layer)\n\n    return layer\n\n# we'll use num_blocks = 3 for this project\ndef vgg_net(input_data, num_blocks=3, max_pool=True):\n    num_filter = 64\n    data = vgg_pool_block(input_data, num_filter, max_pool=max_pool)\n    \n    # run convolution blocks\n    for i in range(2, num_blocks+1):        \n        num_filter *= 2\n        data = vgg_pool_block(data, num_filter, max_pool=max_pool)\n    \n    return LeakyReLU()(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **VGGNet Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def epitope_net(input_data):\n    num_filter = 64\n    data = vgg_pool_block(input_data, num_filter)\n    data = LeakyReLU()(data)\n    \n    info_channels = 10\n    data = Conv2D(info_channels,\n                  (1, 1),\n                  strides=(1, 1),\n                  use_bias=False,\n                  kernel_regularizer=l2(l=0.1),\n                  kernel_initializer=glorot_uniform(),\n                  padding='valid')(data)\n\n    data = BatchNormalization()(data)\n    data = LeakyReLU()(data)\n    return data\n\ndef protein_net(input_data):\n    num_filter = 64\n    data = vgg_pool_block(input_data, num_filter)\n    data = vgg_pool_block(data, num_filter*2)\n    \n    info_channels = 10\n    data = Conv2D(info_channels,\n                  (1, 1),\n                  strides=(1, 2),\n                  use_bias=False,\n                  kernel_regularizer=l2(l=0.1),\n                  kernel_initializer=glorot_uniform(),\n                  padding='valid')(data)\n    data = BatchNormalization()(data)\n    data = LeakyReLU()(data)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNet Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNow we can start with the ResNet code\n\nThe idea of a Residual network is that the input to each convolutional block is added as a \"residual\" to its output. \nIn other words, information from earlier parts of the network \"skips\" to later parts of the network.\n\nWe use the same overall design as vgg, but making each convolutional block into a residual block.\n'''\n\nfrom keras import Sequential, Model\nfrom keras.activations import relu\nfrom keras.layers import *\n\nclass Residual(Model):  #@save\n    \"\"\"The Residual block of ResNet.\"\"\"\n    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = Conv2D(num_channels, padding='same', kernel_size=3, strides=strides)\n        self.conv2 = Conv2D(num_channels, kernel_size=3, padding='same')\n        self.conv3 = None\n        if use_1x1conv:\n            self.conv3 = Conv2D(num_channels, kernel_size=1, strides=strides)\n        self.bn1 = BatchNormalization()\n        self.bn2 = BatchNormalization()\n        self.num_channels = num_channels\n\n    def call(self, X):\n\n        Y = relu(self.bn1(self.conv1(X)))\n        \n        Y = self.bn2(self.conv2(Y))\n        \n        if self.conv3 is not None:\n            X = self.conv3(X)\n        \n        Y += X\n        \n        return relu(Y)\n\nclass ResnetBlock(Layer):\n    def __init__(self, num_channels, num_residuals, first_block=False,\n                 **kwargs):\n        super(ResnetBlock, self).__init__(**kwargs)\n\n        self.num_channels = num_channels\n        self.num_residuals = num_residuals\n        self.first_block = first_block\n        \n        self.residual_layers = []\n        for i in range(num_residuals):\n            if i == 0 and not first_block:\n                self.residual_layers.append(\n                    Residual(num_channels, use_1x1conv=True, strides=2))\n            else:\n                self.residual_layers.append(Residual(num_channels))\n\n    def get_config(self):\n        \n        config = super().get_config().copy()\n        config.update({\n            #'residual_layers': self.residual_layers,\n            'num_channels': self.num_channels,\n            'num_residuals': self.num_residuals,\n            'first_block': self.first_block\n        })\n        \n        return config\n\n    def call(self, X):\n        for layer in self.residual_layers.layers:\n            X = layer(X)\n        return X\n\ndef res_net(small=False):\n    # 3 convolutional blocks hardcoded\n    return Sequential([\n        Conv2D(64, kernel_size=(1,3), strides=(1,1), padding='same'),\n        BatchNormalization(),\n        Activation('relu'),\n        MaxPool2D(pool_size=(1,2), strides=(1,1), padding='same'),\n        # created earlier\n        ResnetBlock(64, 2, first_block=True),\n        ResnetBlock(128, 2),\n        ResnetBlock(256, 2),\n        GlobalAvgPool2D(),\n        Dense(units=10)])\n\ndef res_net_new(x, small=False):\n\n    \n    x = Conv2D(64, kernel_size=(1,3), strides=(1,1), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPool2D(pool_size=(1,2))(x)\n    \n    \n    x = ResnetBlock(64, 2, first_block=True)(x)\n    x = ResnetBlock(128, 2)(x)\n    \n    if (not small):\n        x = ResnetBlock(256, 2)(x)\n        \n        \n    x = GlobalAvgPool2D()(x)\n    \n    x = Dense(units=10)(x)\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Context Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract features from the data\n\ndef context_extract_net(x):\n    \n    x = ZeroPadding2D((0, 2), data_format='channels_last')(x)\n\n    x = LocallyConnected2D(512, \n                           (1, 2),\n                           strides=(1, 1),\n                           use_bias=False,\n                           kernel_regularizer=l2(l=0.1),\n                           kernel_initializer=glorot_uniform(),\n                           padding='valid')(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n\n    x = LocallyConnected2D(512, \n                           (1, 2),\n                           strides=(1, 1),\n                           use_bias=False,\n                           kernel_regularizer=l2(l=0.1),\n                           kernel_initializer=glorot_uniform(),\n                           padding='valid')(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    x = MaxPooling2D(pool_size=(1, 2))(x)\n    x = Dropout(0.5)(x)\n    x = Flatten()(x)\n    return x\n\n# configure the convolutional model\ndef set_model(flag):\n\n    seq_len = 34\n    ligand_len = 9\n\n    pseudo_input = Input(shape=(1, seq_len, 21), name=\"hla\")\n    ligand_input = Input(shape=(1, ligand_len, 21), name=\"epitope\")\n    \n    if flag=='res':\n        \n        pseudo_net = res_net_new(pseudo_input)\n        \n        ligand_net = res_net_new(ligand_input, small=True)\n    else: # run vgg\n        pseudo_net = protein_net(pseudo_input)\n        \n        # use max pooling for ligand\n        ligand_net = epitope_net(ligand_input)\n            \n    merge = concatenate([pseudo_net, ligand_net])\n    \n    if flag == 'vgg':\n        # we only want this for vgg\n        context = context_extract_net(merge)\n    else:\n        context = merge\n        \n    data = Dropout(0.5)(context)\n    data = Dense(100,\n              name=\"regression_dense\",\n              kernel_regularizer=l2(l=0.1),\n              use_bias=True)(data)\n\n    data = LeakyReLU()(data)\n    data = Dropout(0.5)(data)\n\n    output = Dense(1,\n                   name=\"regression\",\n                   kernel_regularizer=l2(l=0.1),\n                   use_bias=False,\n                   activation=\"sigmoid\")(data)\n    #print(\"dense:\", output.shape)\n    model = Model(inputs=[pseudo_input, ligand_input], outputs=[output])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_vgg():\n    model = set_model('vgg')\n    return model\n\ndef model_resnet():\n    model = set_model(\"res\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNow we train the convolutional model of our choice, specified by 'flag'\n'''\ndef train_model(data, flag):\n  net = set_model(flag)\n  trained = net.fit(data)\n  return trained","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_set(samples, ratio):\n  for _ in range(10):\n    np.random.shuffle(samples)\n  \n  train_count = math.ceil(len(samples) * (1 - ratio))\n  return samples[:train_count], samples[train_count:]\n\ndef to_np(arr):\n  X = []\n  for x in arr:\n    X.append(x)\n  X = np.array(X)\n  return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom sklearn.model_selection import KFold\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.utils import Sequence\nfrom keras.initializers import glorot_uniform\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, mean_squared_error, r2_score\n\n\n## NN Model is a neural net model \n## training data in form of a df with 4 columns: Sequence, HLA, Y_val, Pep_Length\n## y --  either immunogenicity or binding scores\n## y_cat = either binding or else\n## outfile where you want to output data to\n## hla_encoding should be pseudo and NEVER full sequence\ndef train_gen_crossval(nn_model, training_data, y_cat, out_file, hla_encoding_method=\"pseudo\"):\n\n  f = open(out_file, \"w\")\n  # Split data into training, test sets\n  splits = list(KFold(n_splits=5, shuffle=True, random_state=20).split(training_data))\n\n  print('we splitted')\n    \n  for (i, (train_index, test_index)) in enumerate(splits):\n    keras.backend.clear_session()\n    # get train_data, validation_data\n\n    trains = training_data.iloc[train_index,:].values\n    train_samples, validation_samples = split_set(trains, 0.2)\n    print(\"split training set\")\n    \n    test_samples = training_data.iloc[test_index,:].values\n    print(\"got test samples\")\n  \n    # make generator out of train data, batch size=16\n    train_generator = DataGenerator(256, train_samples, y_cat)\n    print(\"made training generator:\", train_generator)\n  \n    # make generator out of test data, batch size =16\n    validation_generator = DataGenerator(256, validation_samples, y_cat)\n    print(\"made validation generator:\", validation_generator)\n\n    # call model()\n    model = nn_model()\n    \n    print(\"made model\")\n    # compile model\n    if (y_cat == 'binding'):\n      model.compile(optimizer=Adam(lr=0.001), loss=[\"mean_squared_error\"], metrics = ['mean_squared_error'])\n    \n    else:\n      model.compile(optimizer=Adam(lr=0.001), loss=['binary_crossentropy'],\n                    metrics=[keras.metrics.BinaryAccuracy(), \n                             keras.metrics.TruePositives(),\n                             keras.metrics.TrueNegatives(), \n                             keras.metrics.FalsePositives(),\n                             keras.metrics.FalseNegatives()])\n\n    weights_path = './' + 'weights_{}.h5'.format(i)  \n    print(\"Model compiled\")\n    # checkpoint\n    ckpt = ModelCheckpoint(weights_path, save_best_only=True, \n                           save_weights_only=False, verbose=0, \n                           monitor='val_loss', mode='auto')\n    early = EarlyStopping(monitor='val_loss', patience=5, verbose=0, min_delta=0.001)\n    \n    model.train_generator = train_generator\n\n    model.fit(model.train_generator, epochs=5, \n              steps_per_epoch=len(train_generator), \n              validation_steps=len(validation_generator),\n              validation_data=validation_generator, \n              callbacks=[ckpt, early])\n    # loads the best weights saved by the checkpoint\n    model.load_weights(weights_path)\n\n    hlas = []\n    epitopes = []\n    ys = []\n    xs = []\n\n    for sample in test_samples:\n      hla_encoded = onehot(sample[1], 34).reshape(1, 34, 21)\n      hlas.append(hla_encoded)\n      epitope_encoded = onehot(sample[0], 9).reshape(1, 9, 21)\n      epitopes.append(epitope_encoded)\n      ys.append(sample[3])\n\n    if (y_cat == \"binding\"):\n        results = model.predict({'epitope': np.array(epitopes),\n                             'hla': np.array(hlas)})\n        results = np.array(results)\n        print(results.shape)\n\n        # write results\n        reals = []\n        preds = []\n        bin_reals = []\n        bin_preds = []\n\n        for i in range(len(ys)):\n            real = ys[i]\n            reals.append(ys[i])\n            predict_y = results[i]\n            preds.append(predict_y)\n            if (real < 0.4257):\n                bin_reals.append(0)\n            else:\n                bin_reals.append(1)\n            if (predict_y < 0.4257):\n                bin_preds.append(0)\n            else:\n                bin_preds.append(1)\n\n        reals = np.array(reals)\n        preds = np.array(preds)\n        bin_reals = np.array(bin_reals)\n        bin_preds = np.array(bin_preds)\n\n        auc = roc_auc_score(bin_reals, preds)\n        f1 = f1_score(bin_reals, bin_preds)\n        ppv = precision_score(bin_reals, bin_preds, pos_label=1)\n        mse = mean_squared_error(reals, preds)\n        r2 = r2_score(reals, preds)\n\n        f.write(\"AUC: {},F1: {}, PPV:{}, MSE:{},R2: {}\\n\".format(auc, f1, ppv, mse, r2))\n\n        print(\"Wrote scores for fold %d\" % i)\n    \n    else: #immunogenicity\n        print(\"testing immunogenicity\")\n        for y in ys: \n            xs.append(1)\n        print((np.array(xs)).shape)\n        results = model.evaluate({'epitope': np.array(epitopes),\n                             'hla': np.array(hlas)}, np.array(ys), verbose=1)\n        loss = results[0]\n        auc = results[1]\n        f.write(\"Cross Entropy: {}, AUC: {}\".format(loss, auc))\n        print(f'Cross Entropy: {loss}, AUC: {auc}\\n')\n        print(\"Wrote scores for fold %d\" % i)\n\n    print(\"Finish fold {}...\".format(i))\n    print('\\n'*8)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train on its Own"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_gen(nn_model, training_data, y_cat, mask=False):\n    print(\"starting training\")\n    keras.backend.clear_session()\n    # get train_data, validation_data\n\n    trains = training_data.values\n    train_samples, validation_samples = split_set(trains, 0.2)\n    print(\"split training set\")\n  \n    # make generator out of train data, batch size=16\n    train_generator = DataGenerator(256, train_samples, y_cat)\n    print(\"made training generator:\", train_generator)\n  \n    # make generator out of test data, batch size =16\n    validation_generator = DataGenerator(256, validation_samples, y_cat)\n    print(\"made validation generator:\", validation_generator)\n\n    # call model()\n    model = nn_model()\n    \n    print(\"made model\")\n        # compile model\n    if (y_cat == 'binding'):\n        model.compile(optimizer=Adam(lr=0.001), loss=[\"mean_squared_error\"], metrics = ['mean_squared_error'])\n    else:\n        model.compile(optimizer=Adam(lr=0.001), loss=['binary_crossentropy'], metrics=[keras.metrics.AUC()])\n\n    weights_path = './' + 'best_lstm_imm_model.h5'\n    print(\"Model compiled\")\n    # checkpoint\n    ckpt = ModelCheckpoint(weights_path, save_best_only=True,save_weights_only=False, verbose=0, monitor='val_loss', mode='auto')\n    \n    early = EarlyStopping(monitor='val_loss', patience=5, verbose=0, min_delta=0.001)\n        \n    model.train_generator = train_generator\n    \n    print(\"Fitting Model..........\")\n    model.fit(model.train_generator, epochs=5, steps_per_epoch=len(train_generator), validation_steps=len(validation_generator),\n              validation_data=validation_generator, callbacks=[ckpt, early])\n        \n    # loads the best weights saved by the checkpoint\n    # model.load_weights(weights_path)\n\n    \n    print('\\n'*8)\n    print(\"Done! :D\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_binding=pd.read_csv(\"../input/binding-train-pseudo/binding_train_pseudo.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_binding = df_train_binding[df_train_binding['Pep_Length'] <= 9]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Call Data Generation/Training Function for VGG Model"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_gen_crossval(model_vgg, df_train_binding, \"binding\", \"test_model_output.txt\", hla_encoding_method=\"pseudo\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen_crossval(model_resnet, df_train_binding, \"binding\", \"test_model_output_resnet.txt\", hla_encoding_method=\"pseudo\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Debug immunogenicity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_immuno = pd.read_csv(\"../input/immunogenicity-train-pseudo/immunogenicity_train_pseudo.csv\",index_col=0)\ndf_train_immuno = df_train_immuno[df_train_immuno['Pep_Length'] <= 9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen_crossval(model_vgg, df_train_immuno, \"immunogenicity\", \"immuno_model_output.txt\", hla_encoding_method=\"pseudo\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen_crossval(model_resnet, df_train_immuno, \"immunogenicity\", \"immuno_resnet_output_final.txt\", hla_encoding_method=\"pseudo\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train ResNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen(model_resnet, df_train_binding, \"binding\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model_path, test_data, out_file):\n    ys = []\n    epitopes = []\n    hlas = []\n\n    model = model_resnet()\n\n    test_samples = test_data.values\n\n    model.load_weights(model_path)\n\n    for sample in test_samples:\n        hla_encoded = onehot(sample[1], 34).reshape(1,34, 21)\n        hlas.append(hla_encoded)\n        epitope_encoded = onehot(sample[0], 9).reshape(1,9, 21)\n        epitopes.append(epitope_encoded)\n        ys.append(sample[3])\n        #print(ys)\n\n    results = model.predict({'epitope': np.array(epitopes),'hla': np.array(hlas)})\n    results = np.array(results)\n\n    # write results\n    preds = []\n    bin_reals = []\n    bin_preds = []\n\n\n    for i in range(len(ys)):\n        real=ys[i]\n        predict_y = results[i]\n        print(ys[i])\n        preds.append(predict_y)\n        if (real < 0.4257):\n            bin_reals.append(0)\n        else:\n            bin_reals.append(1)\n        if (predict_y < 0.4257):\n            bin_preds.append(0)\n        else:\n            bin_preds.append(1)\n\n    preds = np.array(preds)\n    bin_reals = np.array(bin_reals)\n    bin_preds = np.array(bin_preds)\n\n    auc = roc_auc_score(bin_reals, preds)\n    f1 = f1_score(bin_reals, bin_preds)\n    ppv = precision_score(bin_reals, bin_preds, pos_label=1)\n    print(\"AUC: {},F1: {}, PPV:{}\\n\".format(auc, f1, ppv))\n\n    f.write(\"AUC: {}, F1: {}, PPV:{}\\n\".format(auc, f1, ppv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = './best_lstm_imm_model.h5'\n#path = \"../output/kaggle/working/weights_4.h5\"\ndf_dbpep_high = pd.DataFrame(pd.read_csv(\"../input/binding-train-pseudo/binding_train_pseudo.csv\", index_col=0))\ntest(path, df_dbpep_high, \"resnet_test.txt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = './best_lstm_imm_model.h5'\n#path = \"../output/kaggle/working/weights_4.h5\"\ndf_dbpep_high = pd.DataFrame(pd.read_csv(\"../input/immunogenicity-train-pseudo/immunogenicity_train_pseudo.csv\", index_col=0))\ntest(path, df_dbpep_high, \"resnet_test_immuno.txt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import *\nfrom keras.utils.generic_utils import CustomObjectScope\n\ndef main(binding_model, immunogenicity_model, out_file, test_X, test_y=None, \n         hla_encoding_method=\"pseudo\", attention=False):\n  if (attention):\n    with CustomObjectScope({'Attention': Attention}):\n      BA_model = load_model(binding_model)\n      IMM_model = load_model(immunogenicity_model)\n  else:\n    BA_model = load_model(binding_model)\n    IMM_model = load_model(immunogenicity_model)\n\n  outfile = open(\"%s.txt\" % (out_file), \"r\")\n  for sample in test_X:\n    hla = onehot(sample[0])\n    peptide = onehot(sample[1])\n    bind_out = BA_model.predict({\n        'protein': np.array([hla]),\n        'ligand': np.array([peptide]),\n        })\n    imm_out = IMM_model.predict({\n      'protein': np.array([hla]),\n      'ligand': np.array([peptide]), \n    })\n    outfile.write('{},{},{} (log_ic50),{} (binary)'.format(sample[0], sample[1], out[0][0][0], out[1][0][0]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}